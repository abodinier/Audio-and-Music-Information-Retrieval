{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab / M2-DS / Audio and Music Information Retrieval / Automatic Chord Recognition (ACR)\n",
    "\n",
    "- Author: geoffroy.peeters@telecom-paris.fr\n",
    "- Date: 2021/02/11\n",
    "- Version: 1.0\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this lab is to create an Automatic Chord Recognition (ACR) system from scratch.\n",
    "To do so you will \n",
    "- 1) compute the Chromagram (temporal sequence of chroma) presentation of an audio signal\n",
    "- 2) implement a simple hidden Markov model (HMM) which states are the chords to be recognized\n",
    "- 3) decode the most likely succession of chord-states of the HMM given the observed sequence of chroma)\n",
    "\n",
    "To do so, we will use the following flowchart which is made up of a set of functions; each perform a specific task:\n",
    "- ```F_get_stft``` compute the Short-Time-Fourier-Transform (STFT) of an audio signal. \n",
    "- ```F_get_chromagram``` transform the STFT into a spectrogram using the conversion-filters given by ```F_get_notechroma_filtre```.\n",
    "- ```F_compute_obs``` compute the observation/emission probability of the HMM given the chromagram and the chord-templates given by ```F_create_chord_templates```\n",
    "- ```F_viterb_decoding``` performs the Viterbi decoding of the chord sequence of the HMM given the transition probability given by ```F_compute_ptrans```.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_flowchart.png width=800px>\n",
    "\n",
    "## Your task:\n",
    "\n",
    "In the following the main code (global architecture) is provided as well as the results you have to find.\n",
    "YOur task is to fill in the missing parts in the code; i.e. the parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```).\n",
    "\n",
    "## Notes:\n",
    "\n",
    "For clarity and easiness-of-understanding of the code, we will use the following coding rules:\n",
    "\n",
    "**Data type**\n",
    "- a variable ending with ```_d```denotes a python dictionary\n",
    "- a variable ending with ```_l```denotes a python list\n",
    "- a variable ending with ```_dl```denotes a python list of python dictionary\n",
    "- a variable ending with ```_v```denotes a numpy 1D-array or a vector\n",
    "- a variable ending with ```_m```denotes a numpy 2D-array or a matrix\n",
    "\n",
    "**Data unit**\n",
    "- units in which the data are expressed using ```_hz``` for data in Hz, ```_sec``` in seconds, ```_n```in samples.\n",
    "\n",
    "**Other**\n",
    "- user defined functions start with a ```F_```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "import librosa\n",
    "eps = 1e-10\n",
    "my_cmap = plt.cm.get_cmap('gray').reversed()\n",
    "\n",
    "audio_file = './gammepno.wav'\n",
    "#audio_file = './Isophonics_01_-_Help!.mp3'\n",
    "\n",
    "do_student = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_v, sr_hz = librosa.load(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute STFT\n",
    "\n",
    "We first compute the amplitude Short Time Fourier Transform: $|X(k,n)|$.\n",
    "\n",
    "We do this using an analysis window of duration ```L_sec``` and of type hamming (```np.hamming```). \n",
    "We choose the hope-size (i.e. the distance between two successive frames) as 1/3 of the window length.\n",
    "\n",
    "To increase the spectral precision (i.e. the distance in Hz between two adjacent DFT frequencies $f_k$ and $f_{k-1}$), we will a large **zero-padding**, i.e. we choose the size ```N``` of the DFT as ```8*nextpow2()``` the size in samples of the window in samples ```L_n```.\n",
    "\n",
    "For this function, we need to perform successively the following step: convert the window size in samples, compute the analysis window, compute the value of ```N```, get the total number of frames that fit in the signal length, allocate matrix that will store the STFT, loop over the frames. \n",
    "\n",
    "The DFT can be computed using ```np.fft.rfft```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_nextpow2(i):\n",
    "    \"\"\"\n",
    "    Find 2^n that is equal to or greater than.\n",
    "    \"\"\"\n",
    "    N = 1\n",
    "    while N < i:\n",
    "        N *= 2\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_get_stft(audio_v, sr_hz, L_sec):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        compute the amplitude of the short-time-fourier-transform using a given window duration L_sec\n",
    "    inputs:\n",
    "        audio_v: audio signal data\n",
    "        sr_hz: sampling rate of audio signal [in Hz]\n",
    "        L_sec: duration of the analysis window [in second]\n",
    "    outputs:\n",
    "        am_fft_m (N/2+1, nb_frame): numpy matrix which stores the amplitude of the STFT \n",
    "            (because of symetry we only keep the positive axis, hence N/2+1)\n",
    "        freq_hz_v (N/2+1): numpy vector which stores the frequencies of the DFT [in Hz]\n",
    "        time_sec_v (nb_frame): numpy vector which stores the middle-time positions of the STFT frames [in second]\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "        \n",
    "    return am_fft_m, freq_hz_v, time_sec_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We now test the ```F_get_stft```function. \n",
    "\n",
    "For the audio signal ```gammepno.wav```, you should obtain the following numbers and figure.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_stft.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_fft_m, freq_hz_v, time_sec_v = F_get_stft(audio_v, sr_hz, L_sec=0.3)\n",
    "print( am_fft_m.shape )\n",
    "print( freq_hz_v.shape )\n",
    "print( time_sec_v.shape )\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "ax.imshow( np.log(1+1e6*am_fft_m), aspect='auto', origin='lower', cmap=my_cmap, extent=(time_sec_v[0], time_sec_v[-1], freq_hz_v[0], freq_hz_v[-1]))\n",
    "ax.set_xlabel('Time [sec]'), ax.set_ylabel('Frequency [Hz]')\n",
    "ax.set_xlim((0, 10))\n",
    "ax.set_ylim((0, 5e3))\n",
    "ax.set_title(audio_file)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Chromagram from STFT\n",
    "\n",
    "We now convert the STFT to a Chromagram. The Chromagram has the same time dimension has the STFT (number of frames) but it maps the frequencies $f_k$ of the STFT to the 12 chroma-values (C,C#,D,...).\n",
    "We do this in two steps.\n",
    "\n",
    "- 1) ```F_get_notechroma_filtre```: we first create a conversion-matrix that will map the SFTF frequencies $f_k$ to the 12 chroma-values. \n",
    "- 2) ```F_get_chromagram```: we mulitply the STFT matrix by the conversion-matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Note and Chroma filters\n",
    "\n",
    "To compute the chroma filters, we \n",
    "- first create the set of note-filters, i.e. the set of filters centered around each midi-pitch\n",
    "- then, for a given chroma, we add all the note-filters that correspond to the same chroma.\n",
    "\n",
    "For the shape of the filter, we will use a simple triangular shape with maximum-value of 1 on the midi-note, 0.5 between two midi-notes and 0 on the previous and next midi note.\n",
    "\n",
    "To create the filters, \n",
    "- we consider all the midi-notes between ```midi_min``` and ```midi_max```\n",
    "- to create the note-filters, we convert the STFT frequencies $f_k$ (```freq_hz_v```) to a midi-scale (using ```F_hz2midi```) and compute the triangular shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_hz2midi(freq, tuning_hz=440): \n",
    "    \"\"\"\n",
    "    convert frequency in Hz to midi value (440Hz -> midi=69)\n",
    "    \"\"\"\n",
    "    \n",
    "    return 12*np.log2((freq+eps)/tuning_hz) + 69\n",
    "\n",
    "\n",
    "def F_midi2hz(midi, tuning_hz=440): \n",
    "    \"\"\"\n",
    "    convert midi value to Hz (midi=69 -> 440Hz)\n",
    "    \"\"\"\n",
    "    \n",
    "    return tuning_hz * (2**((midi - 69)/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_get_notechroma_filtre(freq_hz_v, tuning_hz=440):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        compute the note-filters and chroma-filters\n",
    "    inputs:\n",
    "        freq_hz_v (N/2+1): numpy vector which stores the frequencies of the DFT [in Hz]\n",
    "        tuning_hz: optinal tuning (default is 440 Hz)\n",
    "    outputs:\n",
    "        chroma_filtre_m (N/2+1, 12): numpy matrix which stores the 12 filters to map the DFT frequencies to chroma\n",
    "        note_filtre_m (N/2+1, nb_midi_note): numpy matrix which stores the nb_midi_note filters to map the DFT frequencies to midi_note\n",
    "        midi_v (nb_midi_note): numpy vector which stores the value of the midi notes\n",
    "    \"\"\"\n",
    "    \n",
    "    midi_min = np.round(F_hz2midi(100))\n",
    "    midi_max = np.round(F_hz2midi(4000))\n",
    "\n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "    return chroma_filtre_m, note_filtre_m, midi_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_get_notechroma_filtre```function. \n",
    "\n",
    "For the audio signal ```gammepno.wav```, you should obtain the following numbers and figure.\n",
    "\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_chroma_filtre.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_filtre_m, note_filtre_m, midi_v = F_get_notechroma_filtre(freq_hz_v)\n",
    "print( chroma_filtre_m.shape )\n",
    "print( note_filtre_m.shape )\n",
    "print( midi_v.shape )\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "ax[0].plot(freq_hz_v, note_filtre_m[:, np.where(midi_v==69)[0]], '.')\n",
    "ax[0].set_xlabel('Frequency [Hz]'); ax[0].set_xlim((400,500)); \n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].imshow(np.sqrt(note_filtre_m), aspect='auto', origin='lower', cmap=my_cmap, extent=(midi_v[0], midi_v[-1], freq_hz_v[0], freq_hz_v[-1]))\n",
    "ax[1].set_xlabel('Note Filter');\n",
    "ax[1].set_ylabel('Frequency [Hz]'); ax[1].set_ylim((0,4000))\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[2].imshow(np.sqrt(chroma_filtre_m), aspect='auto', origin='lower', cmap=my_cmap, extent=(0, 12, freq_hz_v[0], freq_hz_v[-1]))\n",
    "ax[2].set_xlabel('Chroma Filter');\n",
    "ax[2].set_ylabel('Frequency [Hz]'); ax[2].set_ylim((0,4000))\n",
    "ax[2].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert STFT to Chromagram using the Chroma filters\n",
    "\n",
    "To convert the STFT ```am_fft_m``` to a Chromagram ```chromagram_m```, we simply multiply the STFT matrix by the conversion-matrix ```chroma_filtre_m``` we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_get_chromagram(am_fft_m, chroma_filtre_m):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        compute the chromagram representation givin the STFT and the chroma filter\n",
    "    inputs:\n",
    "        am_fft_m (N/2+1, nb_frame): numpy matrix which stores the amplitude of the STFT \n",
    "        chroma_filtre_m (N/2+1, 12): numpy matrix which stores the 12 filters to map the DFT frequencies to chroma\n",
    "    outputs:\n",
    "        chromagram_m (12, nb_frame): numpy matrix which stores the 12 chroma-values for each time frame\n",
    "    \"\"\"\n",
    "\n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "\n",
    "    return chromagram_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_get_chromagram```function. \n",
    "\n",
    "For the audio signal ```gammepno.wav```, you should obtain the following numbers and figure.\n",
    "\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_chromagram.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromagram_m = F_get_chromagram(am_fft_m, chroma_filtre_m)\n",
    "print( chromagram_m.shape )\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow( chromagram_m, aspect='auto', origin='lower', cmap=my_cmap, extent=(time_sec_v[0], time_sec_v[-1], 0, 12));\n",
    "plt.xlabel('Time [sec]'); plt.ylabel('Chroma number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chord estimation from Chromagram using Hidden Markov Model\n",
    "\n",
    "Given our observations $O_t$ (stored in ```chromagram_m```), we now estimate the chord sequence by decoding (using the Viterbi decoding algorithm) the most-likely succession of states $q_t$ from a Hidden Markov Model (HMM).\n",
    "\n",
    "For this, we first create the HMM.\n",
    "\n",
    "We define the states $S_j$ of the HMM as the 24 possible Major and minor scales, i.e. a Major and a minor chord starting from each of the possible root note C, C#, D, D#, ...\n",
    "\n",
    "To estimate the observation/emission probability $p(O_t|q=S_j)$, we will use a simple template-based method, i.e. we will compare the theoretical chord-pattern of chord $S_j$ to the observed chroma $O_t$. We will do so by simply computing the dot-product between $O_t$ and the chord-pattern of chord $S_j$. \n",
    "At each time $t$, we then normalize over all $j$ to get a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission probability\n",
    "\n",
    "We first start by creating the chord-templates $T_j$ for each possible Major and minor chords. \n",
    "Each chord-template has 12-dimensions (the same as the chroma representation).\n",
    "\n",
    "We first define a prototype chord-templates for Major chords $T_M$ and minor chords $T_m$ by defining the values at which the template should be equal to 1: it is defined as ```[0,4,7]```for Major chords and ```[0,3,7]```for minor chords. \n",
    "This correspong to the root-note (```0```), major or minor third interval (```4``` or ```3```) and fifth interval (```7```).\n",
    "\n",
    "Using these prototypes, we then create the chord-templates $T_j$ for the 12 possible root-note (C, C#, D, D#, ...). \n",
    "We simply do this by circular permutation of prototype-chord-template.\n",
    "\n",
    "We thus have 2 (Major, minor) * 12 (C, C#, D, D#, ...) = 24 chord-templates.\n",
    "\n",
    "We also add a non-chord (```N```) which is an empty chord-pattern. \n",
    "The goal of the later is to allow easy comparison with ground-truth annotated data which contains the ```N```chord-label.\n",
    "\n",
    "**Advanced students**: you can test adding new prototype chords such as maj7, dom7, min7: ```[0,4,7,11]```, ```[0,4,7,10]```, ```[0,3,7,10]```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chord templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_create_chord_templates():\n",
    "    \"\"\"\n",
    "    description:\n",
    "         compute chord templates for each chord type (major, minor)\n",
    "    inputs:\n",
    "    \n",
    "    outputs:\n",
    "        chord_template_m (nb_chord, 12): numpy matrix which stores the theoretical value of the 12-chromas for each chord\n",
    "        chord_name_l (nb_chord): list of chord names\n",
    "        root_name_l (12): list of root-note names\n",
    "    \"\"\"\n",
    "    \n",
    "    chordtype_d = {}\n",
    "    chordtype_d[''] = [0,4,7] # --- major\n",
    "    chordtype_d[':min'] = [0,3,7] # --- minor\n",
    "    root_name_l = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "        \n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "     \n",
    "    return chord_template_m, chord_name_l, root_name_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_create_chord_templates```function. \n",
    "\n",
    "You should obtain the following numbers and figure.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_templates.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_template_m, chord_name_l, root_name_l = F_create_chord_templates()\n",
    "print( chord_template_m.shape )\n",
    "print( len(chord_name_l) )\n",
    "print( len(root_name_l) )\n",
    "\n",
    "# ------------------------------------------\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(chord_template_m, aspect='auto', origin='lower', cmap=my_cmap)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Chroma'); plt.xticks(np.arange(0,12), root_name_l);\n",
    "plt.ylabel('Chord labels'); plt.yticks(np.arange(0,len(chord_name_l)), chord_name_l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute emission/observation probability as product between chroma and template\n",
    "\n",
    "Using our observations $O_t$ (stored in ```chromagram_m```) and the chord-templates $T_j$ (stored in ```chord_template_m```), we compute the observation/emission probability $p(O_t|q=S_j)$ by simply computing the dot product between $O_t$ and the chord-pattern $T_j$. \n",
    "We then normalize over all $j$ to get a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_compute_pobs(chromagram_m, chord_template_m):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        compute the emission probability of each chord at each time\n",
    "        method: using the dot product between chroma and template\n",
    "        warning: since it is a probability it should be normalized to 1 at each frame\n",
    "    inputs:\n",
    "        chromagram_m (12, nb_frame): numpy matrix which stores the 12 chroma-values for each time frame\n",
    "        chord_template_m (nb_chord, 12): numpy matrix which stores the theoretical value of the 12-chromas for each chord\n",
    "    outputs:\n",
    "        pobs_m (nb_chord, nb_frame): numpy matrix which stores the emission/observation probability of each chord at each time frame\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "\n",
    "    return pobs_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_compute_pobs```function. \n",
    "\n",
    "For the audio signal ```gammepno.wav```, you should obtain the following numbers and figure.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_pobs.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pobs_m = F_compute_pobs(chromagram_m, chord_template_m)\n",
    "print( pobs_m.shape )\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(pobs_m, aspect='auto', origin='lower', cmap=my_cmap, extent=(time_sec_v[0], time_sec_v[-1], 0, len(chord_name_l)));\n",
    "plt.xlabel('Time [sec]'); \n",
    "plt.ylabel('Chord labels'); plt.yticks(np.arange(0,len(chord_name_l))+0.5, chord_name_l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrix\n",
    "\n",
    "So far, there is nothing of a Hidden Markov Model because we still need to define some transition probability $a_{ij} = P(q_{t}=S_j | q_{t-1}=S_i)$, i.e. the probability to transit from a chord $i$ at time $(t-1)$ to a chord $j$ at time $t$.\n",
    "\n",
    "There are many ways to define the transition probabilities between chords (see Lecture 1).\n",
    "\n",
    "To define **how likely it is to have a chord $j$ after a chord $i$**, we use the similarity between chords $i$ and $j$.\n",
    "This similarity is computed by the dot-product between their respective chord-templates $T_i$ and $T_j$.\n",
    "\n",
    "Note that this is equivalent to the use of the distance between the chords in the **Tonnetz-space** representation (see Lecture 1).\n",
    "\n",
    "**Normalization:** We of course need to normalize this value to make a probability: $\\sum_j a_{ij} = 1$.\n",
    "\n",
    "**Trick:** to improve the transition-matrix and reduce the probabiliy of transiting from $i$ to $j \\neq i$, we will exponent $a_{ij}$ by ```**3``` before normalization. \n",
    "This will increase the probability of self-transition and thus reduce the number of state-skip or fragmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_compute_ptrans(chord_template_m):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        compute the transition probability p(q_{t+1}=S_j | q_{t}=S_i) stored as a matrix a_{ij}\n",
    "        method: we use the dot product between chord templates i and j\n",
    "        warning: since it is a probability it should be normalized over j for each i\n",
    "    inputs:\n",
    "        chord_template_m (nb_chord, 12): numpy matrix which stores the theoretical value of the 12-chromas for each chord\n",
    "    outputs:\n",
    "        ptrans_m (nb_chord, nb_chord): numpy matrix which stores the transition probability a_{ij} between chords\n",
    "    \"\"\"\n",
    "   \n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "\n",
    "    return ptrans_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_create_chord_templates```function. \n",
    "\n",
    "You should obtain the following numbers and figure.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_ptrans.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrans_m = F_compute_ptrans(chord_template_m)\n",
    "#ptrans2_m = F_compute_ptrans_circle_of_fifth(chord_name_l)\n",
    "\n",
    "print( ptrans_m.shape )\n",
    "plt.figure(figsize=(14, 10));\n",
    "plt.imshow(ptrans_m, aspect='auto', origin='lower', cmap=my_cmap);\n",
    "plt.grid(True);\n",
    "plt.xticks(np.arange(0,len(chord_name_l)), chord_name_l); plt.xlabel('q_{t+1}=S_j');\n",
    "plt.yticks(np.arange(0,len(chord_name_l)), chord_name_l); plt.ylabel('q_{t}=S_i');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding algorithm\n",
    "\n",
    "Given the prior probability (```prior_v```), the observation/emission probability (```pobs_m```) and the transition probability (```ptrans_m```), we now use the Viterbi decoding algorithm to find the most likely path (```q_v```) over the chord-states.\n",
    "\n",
    "We do this by first performing the ```forward-pass``` using the $\\delta_t(i)$ variable, selecting the highest $q^*_T = \\arg\\max_i \\delta_T(i)$ at the final time and then performing the ```backward-pass``` from its corresponding $\\psi_T(q^*_T)$.\n",
    "\n",
    "**See slides of Lecture 1 (slide 116) for details.**\n",
    "\n",
    "To implement this (and avoid numerical errors), it is highly recommended that you use ```log-probabilities``` instead of probabilities:\n",
    "\n",
    "$$\\delta_1(i) = \\pi_i B_1(X_1) \\Longrightarrow log(\\delta_1(i)) = log(\\pi_i) + \\log(B_1(X_1))$$\n",
    "\n",
    "This will not change the ranking, hence \n",
    "\n",
    "$$\\max_i ( \\delta_{t-1}(i) \\; a_{ij} ) = \\max_i ( \\log(\\delta_{t-1}(i)) + \\log(a_{ij}) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_viterbi_decoding(prior_v, pobs_m, ptrans_m):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        get the most likely path q_v other time using the Viterbi decoding algorithm\n",
    "    inputs:\n",
    "        prior_v (nb_chord): numpy vector which stores the prior probability of each chord\n",
    "        pobs_m (nb_chord, nb_frame): numpy matrix which stores the emission/observation probability of each chord at each time frame\n",
    "        ptrans_m (nb_chord, nb_chord): numpy matrix which stores the transition probability a_{ij} between chords\n",
    "    outpus:\n",
    "        q_v (nb_frame): numpy vector which stores the most likely temporal path over chords\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        # --- START CODE HERE \n",
    "        ...\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "    return q_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We now test the ```F_viterbi_decoding```function. \n",
    "\n",
    "For this, we now switch to the music track \"Help !\" from The Beatles.\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//chord_help.jpg width=200px>\n",
    "\n",
    "Its audio signal is in the file ```Isophonics_01_-_Help!.mp3```.\n",
    "\n",
    "In the following figure (you should obtain the same), we compare \n",
    "- in green: the frame-based approach (which takes the most likely chord at each frame individually $\\arg \\max_j $p(O_t|q=S_j)$).\n",
    "- in red: the viterbi decoding chord sequence ```q_v``` (which uses the transition probability) to \n",
    "\n",
    "Those roughly correspond to the frame-based approach of ```[Fujishima, 1999]``` and the HMM-based approach of  ```[Papadopoulos, Peeters, 2017]``` and the   we saw in Lecture 1.\n",
    "\n",
    "As you can see in the Figure, the chord sequence obtained with the HMM (red) is much better (much less fragemented) than the one obtained with a frame-based decision (red).\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_decoding.png width=800px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HMM-based approach\n",
    "# --- define a uniform prior probability\n",
    "nb_chord = len(chord_name_l)\n",
    "prior_v = np.ones(nb_chord) / nb_chord\n",
    "viterbi_path_v = F_viterbi_decoding(prior_v, pobs_m, ptrans_m)\n",
    "\n",
    "# --- Frame-based approach\n",
    "argmax_path_v = np.argmax(pobs_m, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 10));\n",
    "plt.imshow(pobs_m, aspect='auto', origin='lower', cmap=my_cmap, extent=(time_sec_v[0], time_sec_v[-1], 0, len(chord_name_l)));\n",
    "plt.plot(time_sec_v, argmax_path_v, 'b.')\n",
    "plt.plot(time_sec_v, viterbi_path_v, 'r.')\n",
    "plt.xlabel('Time [sec]'); \n",
    "plt.ylabel('Chord labels'); \n",
    "plt.yticks(np.arange(0,len(chord_name_l))+0.5, chord_name_l);\n",
    "plt.title(audio_file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the performance\n",
    "\n",
    "We will now measure the performance  of our system by comparing its performances to the ground-truth chord annotation ```[given by the Isophonics dataset, Queen Mary University of London]```.\n",
    "\n",
    "We first load the annotations and check that the ```chord-dictionary``` of the annotations correspond to the one of our system, if not, we simply map the chord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_filename = './Isophonics_01_-_Help!.jams'\n",
    "\n",
    "with open(annotation_filename) as fid: annot_d = json.load(fid)\n",
    "chord_annot_dl = annot_d['annotations'][0]['data']\n",
    "\n",
    "# --- Replace annotated-chord-labels which are outside our system-chord-vocabulary\n",
    "for chord_annot_d in chord_annot_dl:\n",
    "    chord_annot_d['value'] = chord_annot_d['value'].replace(':maj6','')\n",
    "\n",
    "set([chord_annot_d['value'] for chord_annot_d in chord_annot_dl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then visually compare our systems prediction to the ground-truth.\n",
    "\n",
    "<img src=https://perso.telecom-paristech.fr/gpeeters/doc/M2DS_-AMIR-LabChordRecognition-20202021_figure//lab_chord_groundtruth.png width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(time_sec_v, argmax_path_v, 'b.')\n",
    "plt.plot(time_sec_v, viterbi_path_v+0.3, 'r.')\n",
    "\n",
    "groundtruth_v = np.zeros(len(time_sec_v))\n",
    "for chord_annot in chord_annot_dl:\n",
    "    start = chord_annot['time']\n",
    "    end = chord_annot['time']+chord_annot['duration']\n",
    "    value = chord_name_l.index( chord_annot['value'] )\n",
    "    plt.plot( [start, end], [value, value], 'g-', linewidth=10, alpha=0.5 )\n",
    "    \n",
    "    pos_s = np.argmin(np.abs(start - time_sec_v))\n",
    "    pos_e = np.argmin(np.abs(end - time_sec_v))\n",
    "    groundtruth_v[pos_s:pos_e] = value\n",
    "plt.xlabel('Time [sec]'); \n",
    "plt.ylabel('Chord labels'); \n",
    "plt.yticks(np.arange(0,nb_chord), chord_name_l);\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the performances\n",
    "\n",
    "We finally compare the performances obtained using our two systems (frame-based and HMM-based) using the Cord-Symbol-Recall: $CSR=\\frac{\\mbox{total duration of segments where annotation equal estimation}}{\\mbox{total duration of annotated segments}}$.\n",
    "\n",
    "Our **HMM-based** system get a score of 0.7836 which is quiet good for such a simple system. The **Frame-based** system only reach 0.4290."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_OCR = np.sum(groundtruth_v == argmax_path_v)/len(groundtruth_v)\n",
    "hmm_OCR = np.sum(groundtruth_v == viterbi_path_v)/len(groundtruth_v)\n",
    "print('Frame-based OCR', hmm_OCR)\n",
    "print('HMM-based OCR', frame_OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
